import bs4
import requests
import csv

all_unit_link = []
main_page_links = []

base_address = "https://www.dailyjanakantha.com/"

def get_sub_links(main_page_link):

    global all_unit_link

    page = requests.get(main_page_link)
    soup = bs4.BeautifulSoup(page.content, 'html.parser')
    newsArticleDivs = soup.find_all("div", {"class": "list-article"})

    for newsArticle in newsArticleDivs:
        a_tag = newsArticle.find("a")
        news_link = a_tag['href']
        complete_url = base_address + news_link[1:]
        print(complete_url)
        all_unit_link.append([complete_url])

    pass

def write_csv(list_to_be_inserted):
    with open('national_unit_page_url.csv', mode='w', newline='') as unit_url_list:
        unit_url_writer = csv.writer(unit_url_list, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)

        for url in list_to_be_inserted:
            unit_url_writer.writerow(url)

    unit_url_list.close()

    pass



with open('national_main_page_url.csv') as main_url_csv:
    readCSV = csv.reader(main_url_csv)

    for row in readCSV:
        main_page_links.append(row[0])

print('+---------------------------------------------------------+')

get_sub_links(main_page_links[5])

for main_link in main_page_links:
    get_sub_links(main_link)
    pass

write_csv(all_unit_link)

print(len(all_unit_link))


